{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc0b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:10.491048Z",
     "iopub.status.busy": "2024-01-04T16:58:10.490696Z",
     "iopub.status.idle": "2024-01-04T16:58:24.006637Z",
     "shell.execute_reply": "2024-01-04T16:58:24.005658Z"
    },
    "id": "GXiHvhBl7bYZ",
    "outputId": "2539258c-9a8c-4d8b-eddf-4b1ea24b2f0e",
    "papermill": {
     "duration": 13.527872,
     "end_time": "2024-01-04T16:58:24.009157",
     "exception": false,
     "start_time": "2024-01-04T16:58:10.481285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install easyfsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506dbf3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:24.027235Z",
     "iopub.status.busy": "2024-01-04T16:58:24.026938Z",
     "iopub.status.idle": "2024-01-04T16:58:35.559938Z",
     "shell.execute_reply": "2024-01-04T16:58:35.558765Z"
    },
    "id": "9uMymFgV7jQO",
    "outputId": "b6f28bd2-8fa1-4e8f-9d30-0b0e134e45a7",
    "papermill": {
     "duration": 11.544693,
     "end_time": "2024-01-04T16:58:35.562510",
     "exception": false,
     "start_time": "2024-01-04T16:58:24.017817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fb7235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:35.581170Z",
     "iopub.status.busy": "2024-01-04T16:58:35.580538Z",
     "iopub.status.idle": "2024-01-04T16:58:35.585217Z",
     "shell.execute_reply": "2024-01-04T16:58:35.584288Z"
    },
    "id": "SsS5sXJf7sFH",
    "papermill": {
     "duration": 0.01623,
     "end_time": "2024-01-04T16:58:35.587362",
     "exception": false,
     "start_time": "2024-01-04T16:58:35.571132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4da35ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:35.605239Z",
     "iopub.status.busy": "2024-01-04T16:58:35.604951Z",
     "iopub.status.idle": "2024-01-04T16:58:39.843856Z",
     "shell.execute_reply": "2024-01-04T16:58:39.843006Z"
    },
    "id": "dhcKeap473Rf",
    "papermill": {
     "duration": 4.25042,
     "end_time": "2024-01-04T16:58:39.846103",
     "exception": false,
     "start_time": "2024-01-04T16:58:35.595683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import plot_images, sliding_average\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c2cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:39.863836Z",
     "iopub.status.busy": "2024-01-04T16:58:39.863391Z",
     "iopub.status.idle": "2024-01-04T16:58:39.926534Z",
     "shell.execute_reply": "2024-01-04T16:58:39.925706Z"
    },
    "id": "wuBVHXd476Ev",
    "outputId": "176daad3-bbe7-41c7-c34d-5e323959a42d",
    "papermill": {
     "duration": 0.074352,
     "end_time": "2024-01-04T16:58:39.928874",
     "exception": false,
     "start_time": "2024-01-04T16:58:39.854522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227d2a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:39.946360Z",
     "iopub.status.busy": "2024-01-04T16:58:39.946074Z",
     "iopub.status.idle": "2024-01-04T16:58:39.953125Z",
     "shell.execute_reply": "2024-01-04T16:58:39.952316Z"
    },
    "id": "1ZJmgrYf8LpG",
    "papermill": {
     "duration": 0.017925,
     "end_time": "2024-01-04T16:58:39.955002",
     "exception": false,
     "start_time": "2024-01-04T16:58:39.937077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LungCancerDataset(Dataset):\n",
    "    def __init__(self, folder_path, label, transform=None, target_size=(256, 256)):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)]\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Resize or crop the image to the target size\n",
    "        image = transforms.functional.resize(image, self.target_size)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319816a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:39.972425Z",
     "iopub.status.busy": "2024-01-04T16:58:39.972150Z",
     "iopub.status.idle": "2024-01-04T16:58:40.461686Z",
     "shell.execute_reply": "2024-01-04T16:58:40.460885Z"
    },
    "id": "K9uhWdF18Udo",
    "papermill": {
     "duration": 0.500942,
     "end_time": "2024-01-04T16:58:40.464051",
     "exception": false,
     "start_time": "2024-01-04T16:58:39.963109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Set your desired image size\n",
    "image_size = 224\n",
    "\n",
    "# Define transforms for the combined dataset\n",
    "combined_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create instances of your custom dataset for each type with labels\n",
    "normal_dataset = LungCancerDataset('/kaggle/input/pneumonia-xray-images/val/normal', label=0, transform=combined_transform)\n",
    "covid_dataset = LungCancerDataset('/kaggle/input/pneumonia-xray-images/val/opacity', label=1, transform=combined_transform)\n",
    "#normal_dataset = LungCancerDataset('/kaggle/input/iqothnccd-lung-cancer-dataset/The IQ-OTHNCCD lung cancer dataset/The IQ-OTHNCCD lung cancer dataset/Normal cases', label=0, transform=combined_transform)\n",
    "# Combine the datasets\n",
    "combined_dataset = ConcatDataset([normal_dataset,covid_dataset])\n",
    "# Create DataLoader instance for the combined dataset\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b9af70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:40.482459Z",
     "iopub.status.busy": "2024-01-04T16:58:40.482132Z",
     "iopub.status.idle": "2024-01-04T16:58:40.493122Z",
     "shell.execute_reply": "2024-01-04T16:58:40.492412Z"
    },
    "id": "s_hE89KS8VfA",
    "papermill": {
     "duration": 0.022202,
     "end_time": "2024-01-04T16:58:40.495127",
     "exception": false,
     "start_time": "2024-01-04T16:58:40.472925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "num_ways = 2\n",
    "num_shots_support = 10\n",
    "num_shots_query = 20\n",
    "batch_size = num_ways*(num_shots_support + num_shots_query)\n",
    "\n",
    "\n",
    "# Calculate the size of the training set and validation set\n",
    "dataset_size = len(combined_dataset)\n",
    "train_ratio = 0.8  # 80% training\n",
    "val_ratio = 0.2    # 20% validation\n",
    "\n",
    "train_size = int(dataset_size * train_ratio)\n",
    "val_size = int(dataset_size * val_ratio)\n",
    "\n",
    "# Create random indices for the training, validation, and test sets\n",
    "indices = list(range(dataset_size))\n",
    "train_indices, val_indices= (\n",
    "    indices[:train_size],\n",
    "    indices[train_size : ],\n",
    ")\n",
    "\n",
    "# Define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "# Create DataLoader instances for the training and validation sets using the samplers\n",
    "train_loader = DataLoader(combined_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(combined_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fab53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:40.513140Z",
     "iopub.status.busy": "2024-01-04T16:58:40.512878Z",
     "iopub.status.idle": "2024-01-04T16:58:40.530785Z",
     "shell.execute_reply": "2024-01-04T16:58:40.530098Z"
    },
    "papermill": {
     "duration": 0.029193,
     "end_time": "2024-01-04T16:58:40.532632",
     "exception": false,
     "start_time": "2024-01-04T16:58:40.503439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from easyfsl.methods.utils import compute_prototypes\n",
    "\n",
    "\n",
    "class FewShotClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract class providing methods usable by all few-shot classification algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: Optional[nn.Module] = None,\n",
    "        use_softmax: bool = False,\n",
    "        feature_centering: Optional[Tensor] = None,\n",
    "        feature_normalization: Optional[float] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Few-Shot Classifier\n",
    "        Args:\n",
    "            backbone: the feature extractor used by the method. Must output a tensor of the\n",
    "                appropriate shape (depending on the method).\n",
    "                If None is passed, the backbone will be initialized as nn.Identity().\n",
    "            use_softmax: whether to return predictions as soft probabilities\n",
    "            feature_centering: a features vector on which to center all computed features.\n",
    "                If None is passed, no centering is performed.\n",
    "            feature_normalization: a value by which to normalize all computed features after centering.\n",
    "                It is used as the p argument in torch.nn.functional.normalize().\n",
    "                If None is passed, no normalization is performed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone if backbone is not None else nn.Identity()\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "        self.prototypes = torch.tensor(())\n",
    "        self.support_features = torch.tensor(())\n",
    "        self.support_labels = torch.tensor(())\n",
    "\n",
    "        self.feature_centering = (\n",
    "            feature_centering if feature_centering is not None else torch.tensor(0)\n",
    "        )\n",
    "        self.feature_normalization = feature_normalization\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        query_images: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict classification labels.\n",
    "        Args:\n",
    "            query_images: images of the query set of shape (n_query, **image_shape)\n",
    "        Returns:\n",
    "            a prediction of classification scores for query images of shape (n_query, n_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"All few-shot algorithms must implement a forward method.\"\n",
    "        )\n",
    "\n",
    "    def process_support_set(\n",
    "        self,\n",
    "        support_images: Tensor,\n",
    "        support_labels: Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Harness information from the support set, so that query labels can later be predicted using a forward call.\n",
    "        The default behaviour shared by most few-shot classifiers is to compute prototypes and store the support set.\n",
    "        Args:\n",
    "            support_images: images of the support set of shape (n_support, **image_shape)\n",
    "            support_labels: labels of support set images of shape (n_support, )\n",
    "        \"\"\"\n",
    "        self.compute_prototypes_and_store_support_set(support_images, support_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_transductive() -> bool:\n",
    "        raise NotImplementedError(\n",
    "            \"All few-shot algorithms must implement a is_transductive method.\"\n",
    "        )\n",
    "\n",
    "    def compute_features(self, images: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute features from images and perform centering and normalization.\n",
    "        Args:\n",
    "            images: images of shape (n_images, **image_shape)\n",
    "        Returns:\n",
    "            features of shape (n_images, feature_dimension)\n",
    "        \"\"\"\n",
    "        original_features = self.backbone(images)\n",
    "        centered_features = original_features - self.feature_centering\n",
    "        if self.feature_normalization is not None:\n",
    "            return nn.functional.normalize(\n",
    "                centered_features, p=self.feature_normalization, dim=1\n",
    "            )\n",
    "        return centered_features\n",
    "\n",
    "    def softmax_if_specified(self, output: Tensor, temperature: float = 1.0) -> Tensor:\n",
    "        \"\"\"\n",
    "        If the option is chosen when the classifier is initialized, we perform a softmax on the\n",
    "        output in order to return soft probabilities.\n",
    "        Args:\n",
    "            output: output of the forward method of shape (n_query, n_classes)\n",
    "            temperature: temperature of the softmax\n",
    "        Returns:\n",
    "            output as it was, or output as soft probabilities, of shape (n_query, n_classes)\n",
    "        \"\"\"\n",
    "        return (temperature * output).softmax(-1) if self.use_softmax else output\n",
    "\n",
    "    def l2_distance_to_prototypes(self, samples: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute prediction logits from their euclidean distance to support set prototypes.\n",
    "        Args:\n",
    "            samples: features of the items to classify of shape (n_samples, feature_dimension)\n",
    "        Returns:\n",
    "            prediction logits of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        return -torch.cdist(samples, self.prototypes)\n",
    "\n",
    "    def cosine_distance_to_prototypes(self, samples) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute prediction logits from their cosine distance to support set prototypes.\n",
    "        Args:\n",
    "            samples: features of the items to classify of shape (n_samples, feature_dimension)\n",
    "        Returns:\n",
    "            prediction logits of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            nn.functional.normalize(samples, dim=1)\n",
    "            @ nn.functional.normalize(self.prototypes, dim=1).T\n",
    "        )\n",
    "\n",
    "    def compute_prototypes_and_store_support_set(\n",
    "        self,\n",
    "        support_images: Tensor,\n",
    "        support_labels: Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract support features, compute prototypes, and store support labels, features, and prototypes.\n",
    "        Args:\n",
    "            support_images: images of the support set of shape (n_support, **image_shape)\n",
    "            support_labels: labels of support set images of shape (n_support, )\n",
    "        \"\"\"\n",
    "        self.support_labels = support_labels\n",
    "        self.support_features = self.compute_features(support_images)\n",
    "        self._raise_error_if_features_are_multi_dimensional(self.support_features)\n",
    "        self.prototypes = compute_prototypes(self.support_features, support_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _raise_error_if_features_are_multi_dimensional(features: Tensor):\n",
    "        if len(features.shape) != 2:\n",
    "            raise ValueError(\n",
    "                \"Illegal backbone or feature shape. \"\n",
    "                \"Expected output for an image is a 1-dim tensor.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3447ed9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:40.549798Z",
     "iopub.status.busy": "2024-01-04T16:58:40.549546Z",
     "iopub.status.idle": "2024-01-04T16:58:40.555607Z",
     "shell.execute_reply": "2024-01-04T16:58:40.554778Z"
    },
    "papermill": {
     "duration": 0.016816,
     "end_time": "2024-01-04T16:58:40.557493",
     "exception": false,
     "start_time": "2024-01-04T16:58:40.540677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "class PrototypicalNetworks(FewShotClassifier):\n",
    "    \"\"\"\n",
    "    Jake Snell, Kevin Swersky, and Richard S. Zemel.\n",
    "    \"Prototypical networks for few-shot learning.\" (2017)\n",
    "    https://arxiv.org/abs/1703.05175\n",
    "\n",
    "    Prototypical networks extract feature vectors for both support and query images. Then it\n",
    "    computes the mean of support features for each class (called prototypes), and predict\n",
    "    classification scores for query images based on their euclidean distance to the prototypes.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_images: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Overrides forward method of FewShotClassifier.\n",
    "        Predict query labels based on their distance to class prototypes in the feature space.\n",
    "        Classification scores are the negative of euclidean distances.\n",
    "        \"\"\"\n",
    "        # Extract the features of query images\n",
    "        query_features = self.compute_features(query_images)\n",
    "        self._raise_error_if_features_are_multi_dimensional(query_features)\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        scores = self.l2_distance_to_prototypes(query_features)\n",
    "\n",
    "        return self.softmax_if_specified(scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_transductive() -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a9490f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:40.574885Z",
     "iopub.status.busy": "2024-01-04T16:58:40.574640Z",
     "iopub.status.idle": "2024-01-04T16:58:43.640792Z",
     "shell.execute_reply": "2024-01-04T16:58:43.639956Z"
    },
    "id": "rm68kQ3Z8YZR",
    "papermill": {
     "duration": 3.077778,
     "end_time": "2024-01-04T16:58:43.643415",
     "exception": false,
     "start_time": "2024-01-04T16:58:40.565637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from easyfsl.modules import resnet12\n",
    "\n",
    "\n",
    "convolutional_network = resnet12()\n",
    "few_shot_classifier = PrototypicalNetworks(convolutional_network).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60aa873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:43.661806Z",
     "iopub.status.busy": "2024-01-04T16:58:43.661468Z",
     "iopub.status.idle": "2024-01-04T16:58:43.668132Z",
     "shell.execute_reply": "2024-01-04T16:58:43.667242Z"
    },
    "id": "PFM21M9M8akN",
    "papermill": {
     "duration": 0.018003,
     "end_time": "2024-01-04T16:58:43.670061",
     "exception": false,
     "start_time": "2024-01-04T16:58:43.652058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 30\n",
    "step_size = 2  # You can adjust the step size based on your preferences\n",
    "scheduler_gamma = 0.1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_optimizer = SGD(\n",
    "    few_shot_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-6\n",
    ")\n",
    "train_scheduler = StepLR(\n",
    "    train_optimizer,\n",
    "    step_size=step_size,\n",
    "    gamma=scheduler_gamma,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb370969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:43.687576Z",
     "iopub.status.busy": "2024-01-04T16:58:43.687269Z",
     "iopub.status.idle": "2024-01-04T16:58:43.694975Z",
     "shell.execute_reply": "2024-01-04T16:58:43.694135Z"
    },
    "id": "rx4i1-4-8eLZ",
    "papermill": {
     "duration": 0.018412,
     "end_time": "2024-01-04T16:58:43.696810",
     "exception": false,
     "start_time": "2024-01-04T16:58:43.678398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "def training_epoch(model: FewShotClassifier, data_loader: DataLoader, optimizer: Optimizer):\n",
    "    all_loss = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(data_loader, total=len(data_loader), desc=\"Training\") as tqdm_train:\n",
    "        for batch_index, (images, labels) in enumerate(tqdm_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Assuming images is a tensor of shape (batch_size, channels, height, width)\n",
    "            support_images, query_images = torch.chunk(images, 2, dim=0)\n",
    "            support_labels, query_labels = torch.chunk(labels, 2, dim=0)\n",
    "\n",
    "            model.process_support_set(support_images.to(DEVICE), support_labels.to(DEVICE))\n",
    "            classification_scores = model(query_images.to(DEVICE))\n",
    "\n",
    "            loss = LOSS_FUNCTION(classification_scores, query_labels.to(DEVICE))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ca4257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:43.714390Z",
     "iopub.status.busy": "2024-01-04T16:58:43.714119Z",
     "iopub.status.idle": "2024-01-04T16:58:43.722027Z",
     "shell.execute_reply": "2024-01-04T16:58:43.721311Z"
    },
    "id": "NKa3s4KN8ezI",
    "papermill": {
     "duration": 0.019091,
     "end_time": "2024-01-04T16:58:43.723907",
     "exception": false,
     "start_time": "2024-01-04T16:58:43.704816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model: FewShotClassifier, data_loader: DataLoader, device, use_tqdm=True, tqdm_prefix=\"Evaluation\"):\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # We use a tqdm context to show a progress bar in the logs\n",
    "        with tqdm(\n",
    "            enumerate(data_loader),\n",
    "            total=len(data_loader),\n",
    "            disable=not use_tqdm,\n",
    "            desc=tqdm_prefix,\n",
    "        ) as tqdm_eval:\n",
    "            for batch_index, (images, labels) in tqdm_eval:\n",
    "                # Split the images and labels into support and query sets\n",
    "                support_images, query_images = torch.chunk(images, 2, dim=0)\n",
    "                support_labels, query_labels = torch.chunk(labels, 2, dim=0)\n",
    "                # Process the support set\n",
    "                model.process_support_set(support_images.to(device), support_labels.to(device))\n",
    "\n",
    "                # Get predictions\n",
    "                predictions = model(query_images.to(device))\n",
    "\n",
    "                \n",
    "\n",
    "                # Calculate accuracy for each episode\n",
    "                _, predicted_labels = torch.max(predictions, 1)\n",
    "                correct_predictions += (predicted_labels == query_labels.to(device)).sum().item()\n",
    "                total_predictions += len(query_labels)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc31271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T16:58:43.741096Z",
     "iopub.status.busy": "2024-01-04T16:58:43.740854Z",
     "iopub.status.idle": "2024-01-04T17:04:12.336735Z",
     "shell.execute_reply": "2024-01-04T17:04:12.335764Z"
    },
    "id": "nBmU1bbg8hDc",
    "outputId": "d0228c4f-302b-4930-e02e-f1e01eaad814",
    "papermill": {
     "duration": 328.607138,
     "end_time": "2024-01-04T17:04:12.339146",
     "exception": false,
     "start_time": "2024-01-04T16:58:43.732008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "# Lists to store the values for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "best_state = few_shot_classifier.state_dict()\n",
    "best_training_accuracy = 0.0  # Initialize with 0 as the starting point\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "current_patience = patience\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(few_shot_classifier, train_loader, train_optimizer)\n",
    "    training_accuracy = evaluate(\n",
    "        few_shot_classifier, train_loader, device=DEVICE, use_tqdm=False, tqdm_prefix=\"Training\"\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch}: Average Loss: {average_loss}, Training Accuracy: {training_accuracy}\")\n",
    "\n",
    "    # Append values for plotting\n",
    "    train_losses.append(average_loss)\n",
    "    train_accuracies.append(training_accuracy)\n",
    "\n",
    "    if training_accuracy > best_training_accuracy:\n",
    "        best_training_accuracy = training_accuracy\n",
    "        best_state = few_shot_classifier.state_dict()\n",
    "        current_patience = patience  # Reset patience when a better model is found\n",
    "        print(\" We found a new best model!\")\n",
    "    else:\n",
    "        current_patience -= 1\n",
    "\n",
    "    if current_patience == 0:\n",
    "        print(f\"Early stopping at epoch {epoch} due to lack of improvement.\")\n",
    "        break\n",
    "\n",
    "    train_scheduler.step()\n",
    "\n",
    "# Plot the learning curve on the same graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss and Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Annotate the best training loss point\n",
    "min_loss_index = train_losses.index(min(train_losses))\n",
    "plt.text(min_loss_index + 1, min(train_losses), f'Min Loss: {min(train_losses):.4f}', ha='center', va='bottom', color='b')\n",
    "\n",
    "# Plot training accuracy on the same graph\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy', marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Annotate the best training accuracy point\n",
    "max_acc_index = train_accuracies.index(max(train_accuracies))\n",
    "plt.text(max_acc_index + 1, max(train_accuracies), f'Max Accuracy: {max(train_accuracies):.2%}', ha='center', va='bottom', color='r')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Load the best model state\n",
    "few_shot_classifier.load_state_dict(best_state)\n",
    "print(\"Loaded the best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2908a6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:12.428536Z",
     "iopub.status.busy": "2024-01-04T17:04:12.428091Z",
     "iopub.status.idle": "2024-01-04T17:04:12.437408Z",
     "shell.execute_reply": "2024-01-04T17:04:12.436510Z"
    },
    "id": "gyvFUgiR8j6z",
    "outputId": "2cf6ee3c-027a-4705-8320-9984272068d2",
    "papermill": {
     "duration": 0.056516,
     "end_time": "2024-01-04T17:04:12.439399",
     "exception": false,
     "start_time": "2024-01-04T17:04:12.382883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_classifier.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b555ac1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:12.527992Z",
     "iopub.status.busy": "2024-01-04T17:04:12.527681Z",
     "iopub.status.idle": "2024-01-04T17:04:12.619111Z",
     "shell.execute_reply": "2024-01-04T17:04:12.618169Z"
    },
    "id": "1wupQJNyBoCj",
    "papermill": {
     "duration": 0.137924,
     "end_time": "2024-01-04T17:04:12.621270",
     "exception": false,
     "start_time": "2024-01-04T17:04:12.483346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your desired image size\n",
    "image_size = 224\n",
    "\n",
    "# Define transforms for the combined dataset\n",
    "combined_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create instances of your custom dataset for each type with labels\n",
    "normal1_dataset = LungCancerDataset('/kaggle/input/covid19-image-dataset/Covid19-dataset/train/Normal', label=0, transform=combined_transform)\n",
    "#cancer1_dataset = LungCancerDataset('/kaggle/input/covid19-image-dataset/Covid19-dataset/test/Covid', label=1, transform=combined_transform)\n",
    "cancer_dataset = LungCancerDataset('/kaggle/input/covid19-image-dataset/Covid19-dataset/train/Covid', label=1, transform=combined_transform)\n",
    "# Combine the datasets\n",
    "test_dataset = ConcatDataset([normal1_dataset,cancer_dataset])\n",
    "# Create DataLoader instance for the combined dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "540a9847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:12.709628Z",
     "iopub.status.busy": "2024-01-04T17:04:12.709274Z",
     "iopub.status.idle": "2024-01-04T17:04:12.715211Z",
     "shell.execute_reply": "2024-01-04T17:04:12.714351Z"
    },
    "id": "-5rJ5SqcDtJu",
    "papermill": {
     "duration": 0.052483,
     "end_time": "2024-01-04T17:04:12.717221",
     "exception": false,
     "start_time": "2024-01-04T17:04:12.664738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "indices = list(range(test_size))\n",
    "test_indices = indices[:test_size]\n",
    "\n",
    "indices = list(range(test_size))\n",
    "test_indices = indices[:test_size]\n",
    "\n",
    "# Define the sampler for obtaining test batches\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# Create DataLoader instance for the test set using the sampler\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, sampler=test_sampler, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c062da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:12.806987Z",
     "iopub.status.busy": "2024-01-04T17:04:12.806625Z",
     "iopub.status.idle": "2024-01-04T17:04:14.004298Z",
     "shell.execute_reply": "2024-01-04T17:04:14.003278Z"
    },
    "id": "8IWw6lTtEigZ",
    "papermill": {
     "duration": 1.245022,
     "end_time": "2024-01-04T17:04:14.006626",
     "exception": false,
     "start_time": "2024-01-04T17:04:12.761604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(len(class_names), len(class_names)))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_sensitivity_specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return sensitivity, specificity\n",
    "\n",
    "def debug_testing(model: FewShotClassifier, data_loader: DataLoader, device, class_names, use_tqdm=True, tqdm_prefix=\"Evaluation\"):\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(\n",
    "            enumerate(data_loader),\n",
    "            total=len(data_loader),\n",
    "            disable=not use_tqdm,\n",
    "            desc=tqdm_prefix,\n",
    "        ) as tqdm_eval:\n",
    "            for batch_idx, (images, labels) in tqdm_eval:\n",
    "                support_images, query_images = torch.chunk(images, 2, dim=0)\n",
    "                support_labels, query_labels = torch.chunk(labels, 2, dim=0)\n",
    "\n",
    "                model.process_support_set(support_images.to(device), support_labels.to(device))\n",
    "                predictions = model(query_images.to(device))\n",
    "\n",
    "                _, predicted_labels = torch.max(predictions, 1)\n",
    "                correct_predictions += (predicted_labels == query_labels.to(device)).sum().item()\n",
    "                total_predictions += len(query_labels)\n",
    "\n",
    "                y_true.extend(query_labels.cpu().numpy())\n",
    "                y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "                print(f\"Batch {batch_idx} - Original Labels: {query_labels.cpu().numpy()}, Predicted Labels: {predicted_labels.cpu().numpy()}\")\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names)\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity, specificity = calculate_sensitivity_specificity(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Sensitivity: {sensitivity * 100:.2f}%')\n",
    "    print(f'Specificity: {specificity * 100:.2f}%')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc668b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:14.096704Z",
     "iopub.status.busy": "2024-01-04T17:04:14.095951Z",
     "iopub.status.idle": "2024-01-04T17:04:25.174779Z",
     "shell.execute_reply": "2024-01-04T17:04:25.173255Z"
    },
    "id": "glNzStdrFcsn",
    "outputId": "efd03492-c145-44b7-f61a-0480266168b6",
    "papermill": {
     "duration": 11.126426,
     "end_time": "2024-01-04T17:04:25.177440",
     "exception": false,
     "start_time": "2024-01-04T17:04:14.051014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have already defined FewShotClassifier, test_loader, device, and class_names\n",
    "\n",
    "# Call the debug_testing function\n",
    "accuracy = debug_testing(few_shot_classifier, test_loader, device=DEVICE, class_names=[\"Normal\", \"Covid\"])\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd971a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:25.271636Z",
     "iopub.status.busy": "2024-01-04T17:04:25.271243Z",
     "iopub.status.idle": "2024-01-04T17:04:25.339918Z",
     "shell.execute_reply": "2024-01-04T17:04:25.338920Z"
    },
    "papermill": {
     "duration": 0.117285,
     "end_time": "2024-01-04T17:04:25.341896",
     "exception": false,
     "start_time": "2024-01-04T17:04:25.224611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have a FewShotClassifier named few_shot_classifier\n",
    "\n",
    "# Specify the file path where you want to save the model\n",
    "model_path = \"/kaggle/working/3shot.pth\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(few_shot_classifier.state_dict(), model_path)\n",
    "\n",
    "# Print a message indicating that the model has been saved\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc697d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T17:04:25.433073Z",
     "iopub.status.busy": "2024-01-04T17:04:25.432434Z",
     "iopub.status.idle": "2024-01-04T17:04:25.437867Z",
     "shell.execute_reply": "2024-01-04T17:04:25.436935Z"
    },
    "id": "sl895At6Ftdc",
    "outputId": "a2151f84-ab0b-4f99-bce0-d19f2d307c4b",
    "papermill": {
     "duration": 0.053223,
     "end_time": "2024-01-04T17:04:25.440087",
     "exception": false,
     "start_time": "2024-01-04T17:04:25.386864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming `few_shot_classifier` is your model\n",
    "print(few_shot_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13082969",
   "metadata": {
    "id": "ZT-LddIXgI5r",
    "papermill": {
     "duration": 0.045185,
     "end_time": "2024-01-04T17:04:25.530406",
     "exception": false,
     "start_time": "2024-01-04T17:04:25.485221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 17810,
     "sourceId": 23812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 584020,
     "sourceId": 1069347,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 627146,
     "sourceId": 1117472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 661308,
     "sourceId": 1166777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1217514,
     "sourceId": 2321803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1748489,
     "sourceId": 2882784,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 380.583441,
   "end_time": "2024-01-04T17:04:27.832157",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-04T16:58:07.248716",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
